# 智能系统 Lab1-1 实验文档

本项目是一个基于反向传播算法的可伸缩、易调整神经网络。具体实现了两项任务：

1. 回归：拟合函数 y = sin(x) , x ∈ [−π, π]
2. 多分类：对 12 个手写汉字分类



## 代码基本架构

本项目自顶向下共有三层架构，以类的方式分别实现：BPNetwork、NeuronLayer、Neuron。



### BPNetwork

这个类实现了一个隐含层可伸缩的 BP 网络，可以设置：

- 输入个数
- 隐含层层数
- 每层隐含层各有多少个神经元
- 输出个数
- 各隐含层各神经元 weights
- 各隐含层 bias
- 输出层各神经元 weights
- 输出层 bias

此外，还可以针对回归或分类任务修改输出层激活函数、损失函数。

在这个类中，完成了以下功能：

- 初始化 BP 网络
- 输出网络信息
- 回归、分类两种传播
- 训练
- 计算误差



### NeuronLayer

这个类代表网络中的隐含层或输出层，可以针对不同的任务给出不同类型的输出方法。例如：

- 回归任务中输出层不需要加 sigmoid 激活函数，使用方差作为损失函数。
- 多分类任务中输出层需要加 softmax 激活函数，并使用交叉熵作为损失函数。

同时，多分类任务中的部分求导计算（从损失函数到输出层输出的部分）也在这层中完成。



### Neuron

这个类代表每层中的每个神经元，主要计算大部分输出和求导公式。



### 参数持久化

本项目利用 pickle 库实现参数持久化，训练完成后直接讲整个 BPNetwork 对象输出到文件，以保存参数。



## 回归任务：拟合 sin(x)

- 在这个任务中，隐含层神经元使用 sigmoid 函数作为激活函数；为参数调整效果明显，输出层不加激活函数。
- 每次训练随机生成一对训练用例。
- 使用 matplotlib 绘制函数图像作为参考。
- 测试用例为从 $-\pi$ 到 $\pi$ 步长为 0.01 的若干对数据。
- 平均误差的计算公式为：$\frac 1N \sum_{k=1}^N \left(Y(k) - sin(k)\right)^2$



### 不同网络结构、网络参数的比较

经初步测试，一层隐含层、50 个 neuron，weigths 在 -0.01 到 0.01 范围内，bias 在 -1 到 0 范围内的效果较好。下面简单比较 weights、bias 范围较小、较大两种情况下的训练表现。

- 由于时间问题，无法测试更多的网络结构与参数，优先从较小范围的参数开始训练。
- 在回归任务中读取参数再次测试并不会影响测试结果。

#### 参数情况一：weights、bias 范围较小

- 隐含层、输出层 weights：-0.001到 0.0001随机取值
- 隐含层、输出层 bias：-0.01 到 0 随机取值

1. 训练 10000 次

   平均误差：0.20698787

   <img src="/Users/liuyiqi/Library/Application Support/typora-user-images/image-20201101182709200.png" alt="image-20201101182709200" style="zoom:67%;" />

2. 训练 100000 次

   平均误差：0.01479212

   <img src="/Users/liuyiqi/Library/Application Support/typora-user-images/image-20201101182642174.png" alt="image-20201101182642174" style="zoom:67%;" />

3. 训练 1000000 次

   平均误差：0.02607785

   <img src="/Users/liuyiqi/Library/Application Support/typora-user-images/image-20201101182534590.png" alt="image-20201101182534590" style="zoom:67%;" />

4. 训练 10000000 次

   平均误差：3.14429901e-05

   <img src="/Users/liuyiqi/Library/Application Support/typora-user-images/image-20201031212633259.png" alt="image-20201031212633259" style="zoom:67%;" />



#### 参数情况二：weights、bias 范围较小

- 隐含层、输出层 weights：-0.01到 0.01随机取值
- 隐含层、输出层 bias：-1 到 0 随机取值

1. 训练 100000 次

   平均误差：0.06904464

   <img src="/Users/liuyiqi/Library/Application Support/typora-user-images/image-20201031213755284.png" alt="image-20201031213755284" style="zoom:67%;" />

2. 训练 1000000 次

   平均误差：0.13049339

   <img src="/Users/liuyiqi/Library/Application Support/typora-user-images/image-20201031213625189.png" alt="image-20201031213625189" style="zoom:67%;" />

   > 可以看出在 weights、bias 范围较大的情况下，训练次数越多，训练效果反而越差，因此不进一步增大次数了。

3. 训练 500000 次

   平均误差：0.02680116

   <img src="/Users/liuyiqi/Library/Application Support/typora-user-images/image-20201031215619508.png" alt="image-20201031215619508" style="zoom:67%;" />

   > 在 500000 次附近调整次数训练效果都比较差，考虑是因为初始值范围过大导致的，因此不进一步试验。



#### 小结

可以看出参数范围较大的情况下，虽然可能更容易到达局部最小值，但调整起来更难收敛得更好。更加理想的办法还是取较小范围的参数，加大训练次数以提高准确率。



## 多分类任务：对 12 个手写汉字分类

- 在这个任务中，隐含层神经元依旧使用 sigmoid 函数作为激活函数。输出层加一层 Softmax 激活函数保证归一性，防止输出结果太大、参数调整不理想。
- 使用 PIL 库读取图像并调整为适合测试集输入的形式：
  1. 将图像调整至 28*28 的大小。
  2. 以灰度模式读取图像。
  3. 遍历每个像素点，如果灰度值大于 196 则视为白色像素，反之则视为黑色。将白色设为 1，黑色设为 0，获得一个代表图像的 (28，28) 的二维数组。
  4. 将二维数组拍平为 (28*28) 的一维数组，用于网络输入。
- 测试集输出即为对应位数为 1，其余位数为 0 的长度为 12 的一维数组。
- 对于每个汉字，选用前 600 张图像作为训练集，后 20 张图像作为测试集。



### 不同网络结构、网络参数的比较

经初步测试，一层隐含层、50 个 neuron，weigths 在 -0.01 到 0.01 范围内，bias 在 -1 到 0 范围内的效果较好。

- 由于时间问题，无法测试更多的网络结构与参数，优先从较小范围的参数开始训练。
- 多分类任务中，读取参数再次测试会影响测试结果，因此取三次测试结果作平均值。

#### 参数情况一：weights、bias 范围较小

- 隐含层、输出层 weights：-0.0001到 0.0001随机取值
- 隐含层、输出层 bias：-0.01 到 0 随机取值

| 训练次数（每次训练 12字*600张图） | 测试正确率 （1） | 测试正确率 （2） | 测试正确率 (3) | 平均正确率 |
| --------------------------------- | ---------------- | ---------------- | -------------- | ---------- |
| 8                                 | 78.75%           | 76.25%           | 74.1667%       | 76.3889%   |
| 10                                | 79.5833%         | 80%              | 74.5833%       | 78.0556%   |
| 11                                | 72.9166%         | 74.5833%         | 73.3333%       | 73.6111%   |
| 12                                | 77.0834%         | 81.6667%         | 75.4167%       | 78.0556%   |
| 15                                | 64.1667%         | 61.25%           | 59.1667%       | 61.5278%   |



#### 参数情况二：weighs、bias 范围较大

- 隐含层、输出层 weights：-0.01到 0.01随机取值
- 隐含层、输出层 bias：-1 到 0 随机取值

| 训练次数（每次训练 12字*600张图） | 测试正确率 （1） | 测试正确率 （2） | 测试正确率 (3) | 平均正确率 |
| --------------------------------- | ---------------- | ---------------- | -------------- | ---------- |
| 10                                | 79.1666%         | 78.3333%         | 79.1666%       | 78.8888%   |
| 11                                | 76.6667%         | 75.4167%         | 79.5833%       | 77.2222%   |
| 12                                | 84.5833%         | 82.5%            | 79.1666%       | 82.0833%   |
| 15                                | 71.6667%         | 75%              | 72.5%          | 73.0555%   |



#### 小结

在扩大了参数范围的情况下，各种训练次数的正确率都有小幅度的上升。此时训练次数在 12 次时有比较好的训练效果。



## 对反向传播算法的理解

反向传播算法在正向地获得输出值后，根据损失函数求出的误差值向之前经过的层的各参数求导，根据一定大小的学习率修改各层参数，从而达到比较好的拟合效果。经过实验，有以下几点收获：

1. 在网络搭建正确、训练集大小适合的情况下，并不需要对同一个训练样本训练太多次就可以得到比较好的结果。
2. 参数初始范围较小可以保证不过快地落入局部最小值，但需要更多的训练次数来保证训练效果；范围较大可以节约一定的训练时间，但效果无法保证。
3. 每一层的 bias 也是需要在训练中求导、修正的。
4. 输出层是否要加激活函数，要加什么样的激活函数，是由任务类型决定的。例如回归任务中，如果输出层加了 sigmoid 函数就会导致输出过小，参数调整效果有限；而如果在多分类任务中输出层没有加 softmax 函数，就会导致输出过大，进而导致参数调整比较困难。
5. 同时，多分类任务如果输出层不加 softmax 函数，是没有办法给损失函数一个目标值的。

以下是本项目可以改进的点：

1. 网络部分 numpy 矩阵用得比较少，导致程序效率较低，训练时间（特别是多分类任务）比较长。
2. 多分类任务的正确率还有待提升，需要进一步优化。